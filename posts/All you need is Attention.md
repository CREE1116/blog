# All you need is Attention

2025-04-29 _ 09:33

#논문리뷰


---
title: All you need is Attention
date: 2025-04-29
description: 
tags: [논문, 리뷰]
---

### 0. 읽은이유

일단 llm관련해서 연구를 하고싶으면, 최신 llm에 대해 어느정도 알아야한다고 생각해서 읽기 시작함.

---
### 1. 논문 정보

- **저자:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin
- **학술지/학회:** Advances in Neural Information Processing Systems 30
- **발표 연도:** 2017
- **DOI (Digital Object Identifier):** 

---

### 2. 서론

- **연구 배경 및 중요성:** 기존 seq2seq모델의 한계를 해결하기 위한 새로운 언어모델의 제안한다.
- **주요 연구 질문 또는 목표:** CNN,RNN과 같은 신경망을 사용하지 않고,Attention 메커니즘만을 사용하여 만든 Transformer모델을 제안한다.
- 시퀀셜하게 단어를 예측하던 기존모델에서 벗어나, 문맥 전체 단어들의 관계를 파악하여 결과를 생성하는 새로운 모델을 제안한다.
- **리뷰어의 첫인상 (간략 요약):** 25년도에 읽고있기 때문에, 익숙해진 내용이라고는 생각하지만, 그당시 지금과 같은 llm이 많이 제안되지 않았던것을 생각하면, 당연 필수라고 생각되는 신경망을 제거하는게 얼마나 과감한 시도인지 알수있다.

---

### 3. 본론

#### 3.1 내용 요약
![[images/스크린샷 2025-04-29 오전 9.55.54.png]]
트렌스포머 아키텍처의 구조는 다음과같이, 인코더, 디코더로 나뉜다.

다음은 해당 도식에서 보이는 함수들의 대한 설명이다.
1. Multi-Head Attention
	1. 입력 벡터를 을 h개의 각기 다른 행렬의 곱을 통해 h개로 나눈뒤 각각 어텐션 연산을 수행한 뒤 concat연산을 수행한다. 그 뒤 다시 행렬곱을 통해, 차원을 입력벡터와 동일하게 한다.
	2. ![[images/스크린샷 2025-04-29 오전 10.51.47.png]]
	3. 입력에 적용되는 행렬곱의 W는 모두 **학습가능**한 값 이기 때문에, 단어에 들어있는 어려가지의 정보(감정의정도, 문맥, 화자, 의미등등)을 나누어 학습할수있다. 또한 병렬처리가 가능하기때문에 GPU를 사용하면 학습 시간을 줄일수있다.
2. Feed Forward Network
	1. 선형변환 - 활성화함수(ReLU) - 선형변환 의 형태로 차원을 대폭 늘려, 의미를 풍부하게 한다(?)
	2. 선형변환에 들어가는 가중치 와 편향 모두 **학습가능**한 값이다.
3. Embedding
	1. 단어를 벡터로 변환하는 함수다. 임베딩또한 **학습가능**하지만, 위의 함수들처럼 학습되는것이 아닌, 이미 학습이 끝난 모델을 사용한다.
	2. 출력/입력 임베딩과 확률출력단에 있는 선형변환은 동일한 가중치를 공유하여, 모델크기를 줄이고, 혼동을 줄인다.

다음은 해당 논문에서 어텐션을 활용하는 3가지 방식이다.
1. Encoder-Decoder Attention
	1. 어텐션의 쿼리는 디코더에서, 키와 벨류는 인코더에서 가져온다.
2. Self-Attention
	1. Encoder-Encoder 
		1. 모든 쿼리, 키, 벨류는 이전 레이어의 인코더에서 받아온다.
	2. Decoder-Decoder
		1. 모든 쿼리, 키, 벨류는 이전 레이어의 디코더에서 받아온다.
		2. 다만 디코더의 경우 아직 생성되지 않은 오른쪽의 위치한 단어가 현제 단어에 미치는 영향을 막아야한다.(need to prevent leftward information flow)
		3. 그렇기때문에 이 경우, 현제 단어 우측에 위치하는 단어에 대하여 소프트맥스 함수를 거칠때 모두 -∞으로 치환한다.(이 경우 소프트맥스를 거치면 확률이 0에 수렴하기 때문에)(Masking)

다음은 셀프 어텐션을 사용하는 이유이다.
![[images/스크린샷 2025-04-29 오전 11.40.47.png]]
1. 계산 복잡성
	1. self-attention의 계산복잡성은  O(n^2 * d), 제한된 self-attention은 O(r * n * d) 로 CNN과, RNN의 핵심연산인 Convolutional O(k * n * d^2) ,Recurrent O(n * d^2) 와 비교해 쉬운 연산이다
2. 병렬화 가능성
	1. self-attention은 각 단어에따라 독립적인 연산이기때문에 병렬화가 가능하다.
3. 장거리 의존성간 경로길이(Path Length for Long-Range Dependencies)(?)
	1. 단 한번의 연산으로 모든 단어와의 관계가 연결된다.
#### 3.2 강점 분석

- **참신성 및 독창성:** 기존의 신경망 기반의 언어모델의 단점을 해소하고, 성능면에서도 더 나은 성과
- **이론적 기여도:** 25년 현제 기준 거의 대부분의 llm의 기반논문

#### 3.3 약점 및 개선점 분석

1. 전반적인 자연어 처리를 목적으로 아키텍처이지만, 벤치마크는 번역에 대한 벤치마크만 있는점
2. 개인적으로 아쉬운건.. 기존 RNN, CNN, seq2seq에 대한 내용은 너무 없다.. 읽기 힘들다.

---

### 4. 결론

- **종합 평가:** 향후 자연어처리 모델에 지대한 영향을 준 논문으로, 독창적이고 과감한 접근방식(NN에서의 탈피), 기반지식이 있다면 읽는것조차 명쾌하고 좋은 논문

---
